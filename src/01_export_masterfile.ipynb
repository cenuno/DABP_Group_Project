{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Master File\n",
    "\n",
    "Author: Cristian E. Nuno\n",
    "\n",
    "Date: October 12, 2018\n",
    "\n",
    "Purpose:\n",
    "\n",
    "* unzips .zip files;\n",
    "* reads them as separate data frames;\n",
    "    + for both the `pitfail` and `prevent` worksheets\n",
    "* binds them together into one data frame; and\n",
    "* exports the results as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.6 |Anaconda custom (64-bit)| (default, Aug 26 2018, 16:30:03) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "# see the value of multiple statements at once\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# load necessary packages\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import xlrd\n",
    "\n",
    "# print version of python\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>abb</th>\n",
       "      <th>region</th>\n",
       "      <th>division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>AZ</td>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>AR</td>\n",
       "      <td>South</td>\n",
       "      <td>West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name abb region            division\n",
       "0     Alabama  AL  South  East South Central\n",
       "1      Alaska  AK   West             Pacific\n",
       "2     Arizona  AZ   West            Mountain\n",
       "3    Arkansas  AR  South  West South Central\n",
       "4  California  CA   West             Pacific"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set working directory\n",
    "directory = \"/Users/cristiannuno/Desktop/Advanced_Analytics/DABP_Group_Project/\"\n",
    "raw_data = \"raw_data/\"\n",
    "write_data = \"write_data/\"\n",
    "\n",
    "os.chdir(directory + raw_data)\n",
    "\n",
    "# load in states data frames\n",
    "states = pd.read_csv(\"states.csv\")\n",
    "\n",
    "states.head()\n",
    "\n",
    "# store abbreviations\n",
    "state_abb = states[\"abb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store all files in current wd\n",
    "# sorting the items in the list alpabetically\n",
    "file_names = sorted(os.listdir(\".\"))\n",
    "\n",
    "# create regex to only give back file names that contain .zip\n",
    "my_pattern = re.compile(\".zip$\")\n",
    "\n",
    "# store zip files that satisfy the regex\n",
    "my_zip_files = list(filter(my_pattern.search, file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007_fsa_acres_sum_final.zip\n",
      "2008_fsa_acres_sum_final.zip\n",
      "2009_fsa_acres_sum_final_8.zip\n",
      "2010_fsa_acres_sum_final_6.zip\n",
      "2011_fsa_acres_sum_jan2012.zip\n",
      "2012_fsa_acres_jan_2013.zip\n",
      "2013_fsa_acres_jan_2014.zip\n",
      "2014_fsa_acres_Jan2014.zip\n",
      "2015_fsa_acres_Jan2016_sq19.zip\n",
      "2016_fsa_acres_jan2017_edr32.zip\n",
      "2017_fsa_acres_jan2018.zip\n"
     ]
    }
   ],
   "source": [
    "# unzip each file in my_zip_files\n",
    "for i in my_zip_files:\n",
    "    print(i)\n",
    "    zip = zipfile.ZipFile(file = i, mode = \"r\")\n",
    "    zip.extractall(os.getcwd())\n",
    "    zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting `.xlsx` files\n",
    "\n",
    "After unzipping the files, I've manually opened them to inspect the `.xlsx` files. What I found was that each year contains two worksheets:\n",
    "\n",
    "* `pltfail`: one row per state (including Puerto Rico and the Virgin Islands) representing the planted acres (including failed acres) reported to the Farm Service Agency for a variety of crops; and\n",
    "* `prevent`: one row per state (including Puerto Rico and the Virgin Islands) representing the prevented acres reported to the Farm Service Agency for a variety of crops. \n",
    "\n",
    "This means we'll have two master files: one for planted and another for prevented acres from 2007 to 2017. *Note: not all of the workbooks spell these two sheets the same. However, each one starts with `pltfail`, followed by `prevent`.*\n",
    "\n",
    "### Column Names\n",
    "\n",
    "While the spelling of the column names are not all similar, thankfully, they are all in the correct order. Each year follows this pattern:\n",
    "* `state`: the state name\n",
    "* `barley`: acres for barley\n",
    "* `corn`: acres for corn\n",
    "* `cotton_els`: acres for cotton, extra long staple\n",
    "* `cotton_upland`: acres for cotton, upland\n",
    "* `oats`: acres for oats\n",
    "* `rice`: acres for rice\n",
    "* `sorghum`: acres for sorghum\n",
    "* `sugar_beets`: acres for sugar beets\n",
    "* `sugarcane`: acres for sugarcane\n",
    "* `wheat`: acres for wheat\n",
    "* `total`: sums the acres across all crops\n",
    "\n",
    "There is no need for the `total` column so it'll be dropped.\n",
    "\n",
    "### Note\n",
    "\n",
    "The `.xlsx` files are not normalized in the sense that sheet names are spelled differently and the headers start at different rows. Going to manually import one data frame at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create regex to only give back file names that contain .xlsx\n",
    "my_pattern = re.compile(\".xlsx$\")\n",
    "\n",
    "# store xlsx files that satisfy the regex\n",
    "my_xlsx_files = list(filter(my_pattern.search, file_names))\n",
    "\n",
    "# store the first four digits from each item in the list\n",
    "first_four_digits = []\n",
    "for i in range(0, len(my_xlsx_files)):\n",
    "    first_four_digits.append(int(my_xlsx_files[i][:4]))\n",
    "\n",
    "# store column names\n",
    "standard_col_names = [\"state\", \"barley\", \"corn\"\n",
    "                     , \"cotton_els\", \"cotton_upland\", \"oats\"\n",
    "                     , \"rice\", \"sorghum\", \"soybeans\"\n",
    "                     , \"sugar_beets\", \"sugarcane\", \"wheat\"\n",
    "                     , \"total\"]\n",
    "\n",
    "# store non state names\n",
    "non_states = ['Grand Total', 'US']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each excel file, do the following:\n",
    "* transfrom it into a data frame;\n",
    "* standarize the column names;\n",
    "* create a `year` column;\n",
    "* create a `type` column to identify if this plant and fail data or prevention data;\n",
    "* then row bind all the data frames together in `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007_fsa_acres_summary_final.xlsx is currently being processed...\n",
      "... 2007_fsa_acres_summary_final.xlsx was successfully binded to `df`.\n",
      "2008_fsa_acres_summary_final.xlsx is currently being processed...\n",
      "... 2008_fsa_acres_summary_final.xlsx was successfully binded to `df`.\n",
      "2009_fsa_acres_summary_final_8.xlsx is currently being processed...\n",
      "... 2009_fsa_acres_summary_final_8.xlsx was successfully binded to `df`.\n",
      "2010_fsa_acres_summary_final_6.xlsx is currently being processed...\n",
      "... 2010_fsa_acres_summary_final_6.xlsx was successfully binded to `df`.\n",
      "2011_fsa_acres_sum_jan2012.xlsx is currently being processed...\n",
      "... 2011_fsa_acres_sum_jan2012.xlsx was successfully binded to `df`.\n",
      "2012_fsa_acres_jan_2013.xlsx is currently being processed...\n",
      "... 2012_fsa_acres_jan_2013.xlsx was successfully binded to `df`.\n",
      "2013_fsa_acres_jan_2014.xlsx is currently being processed...\n",
      "... 2013_fsa_acres_jan_2014.xlsx was successfully binded to `df`.\n",
      "2014_fsa_acres_jan2014.xlsx is currently being processed...\n",
      "... 2014_fsa_acres_jan2014.xlsx was successfully binded to `df`.\n",
      "2015_fsa_acres_01052016.xlsx is currently being processed...\n",
      "... 2015_fsa_acres_01052016.xlsx was successfully binded to `df`.\n",
      "2016_fsa_acres_010417.xlsx is currently being processed...\n",
      "... 2016_fsa_acres_010417.xlsx was successfully binded to `df`.\n",
      "2017_fsa_acres_010418.xlsx is currently being processed...\n",
      "... 2017_fsa_acres_010418.xlsx was successfully binded to `df`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1174, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['state', 'barley', 'corn', 'cotton_els', 'cotton_upland', 'oats',\n",
       "       'rice', 'sorghum', 'soybeans', 'sugar_beets', 'sugarcane', 'wheat',\n",
       "       'total', 'year', 'type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2007    110\n",
       "2015    108\n",
       "2010    108\n",
       "2008    108\n",
       "2017    106\n",
       "2016    106\n",
       "2014    106\n",
       "2013    106\n",
       "2012    106\n",
       "2009    106\n",
       "2011    104\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "prevent           587\n",
       "plant and fail    587\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty data frame\n",
    "df = pd.DataFrame()\n",
    "for i in range(0, len(my_xlsx_files)):\n",
    "    # for years 2007 - 2011\n",
    "    # make the header the second row in the file\n",
    "    if i in list(range(0, 5)):\n",
    "        header_row = 1\n",
    "        \n",
    "    # otherwise, make the header the fourth row in the file\n",
    "    else:\n",
    "        header_row = 3\n",
    "    \n",
    "    # print the current file being processed\n",
    "    print(my_xlsx_files[i] + \" is currently being processed...\")\n",
    "    \n",
    "    # read in pltfail sheet\n",
    "    pltfail = pd.read_excel(io = my_xlsx_files[i] \\\n",
    "                         , sheet_name = 0 \\\n",
    "                         , header = header_row)\n",
    "\n",
    "    # standardize column names\n",
    "    pltfail.columns = standard_col_names\n",
    "    \n",
    "    # create the year column\n",
    "    pltfail[\"year\"] = first_four_digits[i]\n",
    "    \n",
    "    # create the type column\n",
    "    pltfail[\"type\"] = \"plant and fail\"\n",
    "    \n",
    "    # read in prevent sheet\n",
    "    prevent = pd.read_excel(io = my_xlsx_files[i] \\\n",
    "                         , sheet_name = 1 \\\n",
    "                         , header = header_row)\n",
    "    \n",
    "    # standardize column names\n",
    "    prevent.columns = standard_col_names\n",
    "    \n",
    "    # create the year column\n",
    "    prevent[\"year\"] = first_four_digits[i]\n",
    "    \n",
    "    # create the type column\n",
    "    prevent[\"type\"] = \"prevent\"\n",
    "    \n",
    "    # append pltfail and prevent to temp\n",
    "    temp = pltfail.append(prevent)\n",
    "    \n",
    "    # append temp to df\n",
    "    df = df.append(temp)\n",
    "    \n",
    "    # print the current file that was binded to df\n",
    "    print(\"... \" + my_xlsx_files[i] + \" was successfully binded to `df`.\")\n",
    "    \n",
    "df.shape\n",
    "df.columns\n",
    "df[\"year\"].value_counts()\n",
    "df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our final data set, let's clean it up a bit:\n",
    "\n",
    "* clean white space in the `state` column and make it title case;\n",
    "* drop the `total` column;\n",
    "* keep all rows where the `state` value doesn't equal `non_states`; and\n",
    "* merge the `state` data frame onto `df_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(200, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cristiannuno/ENTER/lib/python3.5/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(952, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1152, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop total\n",
    "df_clean = df.drop(\"total\", axis = 1)\n",
    "\n",
    "# keep records of only states\n",
    "df_clean = df_clean.query(\"state not in @non_states\")\n",
    "df_clean.shape\n",
    "\n",
    "# keep certain rows\n",
    "df_clean_abb = df_clean.query(\"state in @state_abb\")\n",
    "df_clean_nam = df_clean.query(\"state not in @state_abb\")\n",
    "\n",
    "# join states onto df_clean_abb\n",
    "# remove the original state column\n",
    "# and rename 'name' as state\n",
    "df_clean_abb = pd.merge(df_clean_abb, states \\\n",
    "                        , how = \"left\", left_on = \"state\" \\\n",
    "                        , right_on = \"abb\")\n",
    "\n",
    "df_clean_abb = df_clean_abb.drop(\"state\", axis = 1)\n",
    "\n",
    "df_clean_abb.columns = [\"state\" if x == \"name\" else x for x in df_clean_abb.columns]\n",
    "df_clean_abb.shape\n",
    "\n",
    "# clean the state column prior to merging\n",
    "df_clean_nam[\"state\"] = df_clean_nam[\"state\"].str.strip().str.title()\n",
    "\n",
    "# join states onto df_clean_nam\n",
    "df_clean_nam = pd.merge(df_clean_nam, states \\\n",
    "                        , how = \"left\", left_on = \"state\" \\\n",
    "                        , right_on = \"name\")\n",
    "\n",
    "# drop the name column\n",
    "df_clean_nam = df_clean_nam.drop(\"name\", axis = 1)\n",
    "df_clean_nam.shape\n",
    "\n",
    "# bind the two data frames together\n",
    "df_clean = df_clean_abb.append(df_clean_nam)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data\n",
    "\n",
    "Prior to exporting, rearrange the columns so that the data frame is more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(directory + write_data)\n",
    "\n",
    "# reorder columns\n",
    "# note: can only do this with integer positions rather than column names which is not ideal\n",
    "new_order = [14, 16, 11, 0, 7, 5\n",
    "            , 1, 2, 3, 4, 6, 8\n",
    "            , 9, 10, 12, 13, 15]\n",
    "#new_order = [\"type\", \"year\", \"state\", \"abb\", \"region\", \"division\"\n",
    "#            , \"barley\", \"corn\", \"cotton_els\", \"cotton_upland\", \"oats\", \"rice\"\n",
    "#            , \"sorghum\", \"soybeans\", \"sugar_beets\", \"sugarcane\", \"wheat\"]\n",
    "\n",
    "df_clean = df_clean[df_clean.columns[new_order]]\n",
    "\n",
    "df_clean.to_csv(\"clean_crop_2011_2017.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ENTER]",
   "language": "python",
   "name": "conda-env-ENTER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
